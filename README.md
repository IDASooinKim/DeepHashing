<div align="center">

# Enhancing deephashing with graph filters and autoencoder-based embeddings

![poster](./images/arch.png)
<div align="left">

## Description

*This is a PyTorch implementation of a deep hashing algorithm integrated with a __mini-batch-based graph construction__ module. The provided experiments and dataset use [Stanford Cars](https://ai.stanford.edu/~jkrause/cars/car_dataset.html).
This code implements a basic image retrieval pipeline using the proposed self-supervised deep hashing model, HAGCN. The script takes an embedded dataset of images generated by the backbone model as input. After tuning the model's hyperparameters, the training process allows the model to generate binary hash codes. The code is modular, with separate functions for data loading, preprocessing, feature extraction, model training, and evaluation. In the final part of the script, the model is tested on a test dataset to demonstrate its retrieval capabilities.*




## Usage Instructions & Requrements
Please follow the instructions below to set up the experimental environment.
To train the model, you need to set up the experimental environment. Use a virtual environment such as Anaconda to install the packages listed in the provided requirements.txt file. While various versions of Python modules may be used, the versions specified in the text file reflect the environment used in the experiments.

### 1. Clone the repository

```{shell}
git clone https://github.com/IDASooinKim/DeepHashing.git
```

### 2. Creating conda envs

```{shell}
conda create -n deephashing python=3.8
conda activate deephashing
```

### 3. Install requirements 

```{shell}
pip install -r requirements.txt
```

### 4. Training Data preparation

Please download the image dataset for the experiment. 
The downloaded images must be embedded into 1-dimensional vectors of size 784 using the [ViT B/16](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html) model. 
For faster experimentation, you can download the pre-embedded [Stanford_Cars dataset](https://drive.google.com/file/d/1s39IUmYMnvvwMu1eotckh3HF6Mr1QvUt/view?usp=drive_link).

Each folder in the downloaded dataset represents a class, and each folder contains approximately 50 embeddings of the same class. If you wish to train with custom data, please follow the folder directory structure below.

```
project/
├── data/
│   ├── 0
|   |───── 0.npy
|   |───── 1.npy
│   └── 1
```

### 5. Model Train

Model training can be easily run using the command __python main.py__.
If you wish to adjust the batch size, number of epochs, or other parameters, please use the following command:

```{shell}
python main.py --num_epochs 200 --batch_size 64 --num_cls 10 
```

For detailed arguments, please refer to the arguments.py file.
