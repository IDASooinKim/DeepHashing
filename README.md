<div align="center">

# Enhancing deephashing with graph filters and autoencoder-based embeddings

![poster](./images/arch.png)
<div align="left">

# Description

*This is a PyTorch implementation of a deep hashing algorithm integrated with a __mini-batch-based graph construction__ module. The provided experiments and dataset use [Stanford Cars](https://ai.stanford.edu/~jkrause/cars/car_dataset.html).
This code implements a basic image retrieval pipeline using the proposed self-supervised deep hashing model, HAGCN. The script takes an embedded dataset of images generated by the backbone model as input. After tuning the model's hyperparameters, the training process allows the model to generate binary hash codes. The code is modular, with separate functions for data loading, preprocessing, feature extraction, model training, and evaluation. In the final part of the script, the model is tested on a test dataset to demonstrate its retrieval capabilities.*

# Dataset Information

HAGCN is tested using the benchmark datasets [STL-10](https://cs.stanford.edu/~acoates/stl10/), [Stanford Cars](https://ai.stanford.edu/~jkrause/cars/car_dataset.html), and [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet). 

The STL-10 dataset is a computer vision benchmark with 10 classes of images of size 96 × 96, which is widely used for tasks such as representation learning, deep hashing, and self-supervised learning.
Each image has a resolution of 96×96. 

The Stanford Car dataset contains over 16,000 images across 196 classes, based on car manufacturer and model year. 
The images are of varying resolutions, but most are high-resolution, typically around 300×300 to 600×600 pixels. All images were resized to 224×224. This dataset shows the visual differences in car model designs over time and is used to evaluate models for fine-grained recognition and transfer learning in computer vision. 

The Tiny ImageNet dataset consists of 200 classes and 500 training images per class, and the ImageNet images are downsampled to 64×64. This dataset helps evaluate the scalability and robustness of the model for different object types.

The downloaded images must be embedded into 1-dimensional vectors of size 784 using the [ViT B/16](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html) model.
For faster experimentation, Please download the pre-embedded image dataset for the experiment:  you can download the pre-embedded [Stanford_Cars dataset](https://drive.google.com/file/d/1s39IUmYMnvvwMu1eotckh3HF6Mr1QvUt/view?usp=drive_link).

# Data Preprocessing

The following code is a preprocessing procedure that extracts an image as an embedding vector using the pretrained ViT-B/16 model.

```python
import torch
from torchvision import models, transforms
from PIL import Image

model = models.vit_b_16(pretrained=True)
model.eval()  # Set to evaluation mode

def extract_features(image_tensor):
    # image_tensor: (B, 3, 224, 224)
    with torch.no_grad():
        outputs = model._process_input(image_tensor)  # patch + pos embed
        n = outputs.shape[1]
        cls_token = model.cls_token.expand(image_tensor.shape[0], -1, -1)
        x = torch.cat((cls_token, outputs), dim=1)
        x = model.encoder(x)  # transformer encoder
        return x[:, 0]  # take only the class token output

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNet mean
        std=[0.229, 0.224, 0.225]    # ImageNet std
    )
])

image_path = "your_image.jpg"  # Replace with your actual image path
image = Image.open(image_path).convert("RGB")
image_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension

features = extract_features(image_tensor)
print("Extracted feature shape:", features.shape)  # (1, 768)
```

1. Load and Set Up the Model
> We begin by loading the pretrained vit_b_16 model from torchvision.models. The model is set to evaluation mode to ensure no gradients are computed.

2. Modify the Model to Extract Intermediate Features
> ViT represents the entire image using the output of a special class token from the last transformer layer. We manually pass the input through the embedding and encoder stages to directly access this representation.

3. Preprocess the Image
> ViT models expect input images of size 224×224 and normalized using ImageNet statistics. We use standard torchvision.transforms to prepare the image.

4. Load and Preprocess the Image
> The image is opened with PIL, converted to RGB, and passed through the preprocessing pipeline. A batch dimension is added for model compatibility.

5. Extract Feature Embedding
> Finally, we run the image tensor through the custom extract_features() function to obtain a 768-dimensional embedding vector.

Result
> You now have a 768-dimensional feature vector that represents the global content of your input image — ideal for tasks like image retrieval, clustering, or zero-shot learning.

# Evaluation and Assessment metrics

* Model
> The ViT-B/16 model pretrained on ImageNet-21k (21,841 classes) is used.

* Evaluation Metrics
> All models are evaluated using mAP@k based on hash length, loss variation, data distribution, and model parameters.

* Validation Scheme
> A 5-fold cross-validation is applied on both training and testing datasets to ensure fair comparison.
> The value of k is set equal to the number of classes for all datasets.

* Experimental Setup
> Experiments are conducted using PyTorch on a system with four NVIDIA RTX A5000 GPUs and 128GB RAM.

* Optimization
> The Adam optimizer is used with a momentum of 0.9 and an initial learning rate of 1e-4.
> The trade-off parameter λ is set to 1e-4 based on preliminary experiments analyzing the L1 and L2 loss ratio.

* Training Configuration
> Batch size is set to 128, and hash lengths used are 16, 28, 32, 64, and 128.

# Code Information

This code loads .npy files organized in folders into a custom PyTorch Dataset and splits them into train, test, and query sets.
It sets up a data loading pipeline for training and evaluating image retrieval or hashing models.

### Data loading
```python
class NpyFolderDataset(Dataset):
    def __init__(self, ...)
    ...
    def __len__(self):
    ...
    def __getitem__(self):
    ...
```

* `NpyFolderDataset` Class
  
> A custom PyTorch Dataset class for loading .npy files organized in a folder structure.
> It traverses class-specific directories, collecting file paths and corresponding labels.
> `__len__` method returns the total number of samples in the dataset.

* `__getitem__` Method
  
> Loads the .npy file at the specified index, converts it into a float tensor, and returns it with its label.
> the sample tensor is squeezed to remove extra dimensions.

```python
def get_data_loader():
    ...
    train_test_split()
    ...
    DataLoader()
```

* `get_data_loader` Function
  
> Randomly splits the dataset into training, test (database), and query sets
> and returns `PyTorch DataLoader` objects for each split for efficient batching and loading.

### Model building

> This source file contains the direct implementation of the HAGCN model.
> First, the sigmoid function used for binarizing the hash codes is defined as follows.

```python 
class Asig(nn.Module):
    def forward(self, x, alpha):
        return torch.sigmoid(alpha*x)
```

The following is a description of each function in the proposed HAGCN model.

```python
class Machine(nn.Module):
        ...
    def __init__(self)
        ...
    def adj_generator(self, A, device)
        ...
    def fourier(self, L, k=1)
        ...
    def bspline_basis(self, K, x, degree)
        ...
    def chebyshev_polynomials(self, L_hat, K)
        ...
    def lanczos_algorithm(self, A, k, v0=None)
        ...
    def calc_sim(self, x)
        ...
    def forward(self, x, alpha)
```

* `__init__` Method
  
> Initializes the model structure and hyperparameters, setting different graph filtering weights depending on the task
> and defines the necessary layers, activation functions, and normalization components.

* `adj_generator` Function
  
> Generates a normalized adjacency matrix and diagonal matrix from the input similarity matrix
> then performs basic preprocessing for graph filtering.

* `Fourier` Function
  
> Computes the eigenvalues and eigenvectors of the graph Laplacian matrix.
> Used in Fourier-based graph filtering.

* `Poly` Function
  
> Generates B-spline basis functions for use in spline filtering by bspline_basis function
> Implements the recursive Cox-De Boor algorithm.

* `Cheby` Function
  
> Computes Chebyshev polynomials to create bases for filtering.
> Efficiently handles repeated linear operations on the graph.

* `Lanc` Function
  
> Uses the Lanczos algorithm to approximate a symmetric matrix with a tridiagonal matrix and orthogonal basis.
> Provides the foundation for Lanczos-based filtering.

* `calc_sim` Function
  
> Calculates pairwise similarity between feature vectors to produce a similarity matrix.
> Provides relationship information for subsequent graph operations.

* `forward` Function
  
> Defines the full forward pass of the model, performing task-specific graph filtering.
> Outputs the hash code, similarity matrix, and feature representation.

### Model training

The following is a description of training steps for our experiments

```python
def run_experiment(k, num_epochs, task, exp_hash_len, train_loader, test_loader, query_loader, device)"
    ...
    for hash_len in exp_hash_len:
        for key in task:
            ...
            machine = Machine( ...)
            criterion = nn.MSELoss()  # Mean Squared Error Loss
            optimizer = optim.Adam(machine.parameters(), lr=0.0001)
            ...
            for idx, epoch in enumerate(range(num_epochs)):
                ...
            with torch.no_grad():
                ...
                for x_batch, y_batch in test_loader:
                    ...
                np.where(dataset_hcodes>=0.5, 1, 0)
                ...
                for x_batch, y_batch in query_loader:
                ...
                testset_hcodes = np.where(testset_hcodes>=0.5, 1, 0)
            mAP = CalcTopMap(...)
```

* Model and Experiment Setup
  
> The Machine model is initialized with different combinations of task and hash length.
> Defines the loss function (MSE) and optimizer (Adam).

* Training Loop
  
> Iterates over training data, calculating reconstruction loss and similarity-based hash loss.
> Performs backpropagation to update model parameters.

* Hash Code Binarization
  
> Converts sigmoid outputs into binary hash codes using a 0.5 threshold.
> The binary values are then scaled to {-1, 1} for evaluation.

* Test Set Hash Code Generation
  
> Generates hash codes for the gallery (database) using the test_loader.
> Appends each batch's hash codes and labels to lists, then concatenates them.

* Query Set Hash Code Generation
  
> Generates hash codes for the query set using the query_loader.
> Similarly binarizes and stores hash codes and labels as full arrays.

* Retrieval Performance Evaluation (mAP Calculation)
  
> Computes mAP using CalcTopMap() between query and gallery hash codes.
> Measures retrieval accuracy based on top-k Hamming distance.

* Result Logging and Output
  
> Tracks the best mAP and logs the results to a CSV file.
> Prints a summary of the experiment including task, k, and hash_len.

# Usage Instructions & Requrements
Please follow the instructions below to set up the experimental environment.
To train the model, you need to set up the experimental environment. Use a virtual environment such as Anaconda to install the packages listed in the provided requirements.txt file. While various versions of Python modules may be used, the versions specified in the text file reflect the environment used in the experiments.

### 1. Clone the repository

```{shell}
git clone https://github.com/IDASooinKim/DeepHashing.git
```

### 2. Creating conda envs

```{shell}
conda create -n deephashing python=3.8
conda activate deephashing
```

### 3. Install requirements 

```{shell}
pip install -r requirements.txt
```

### 4. Training Data preparation

Each folder in the downloaded dataset represents a class, and each folder contains approximately 50 embeddings of the same class. If you wish to train with custom data, please follow the folder directory structure below.

```
project/
├── data/
│   ├── 0
|   |───── 0.npy
|   |───── 1.npy
│   └── 1
```

### 5. Model Train

Model training can be easily run using the command __python main.py__.
If you wish to adjust the batch size, number of epochs, or other parameters, please use the following command:

```{shell}
python main.py --num_epochs 200 --batch_size 64 --num_cls 10 
```

For detailed arguments, please refer to the arguments.py file.

# Conclusions and Limitations

## Problem
Traditional CNN-based deep hashing methods rely on fixed graph structures and grid-based convolutions, making it difficult to capture complex and unstructured relationships in high-dimensional data.

## Proposed Solution
The study introduces **HAGCN**, a model that dynamically constructs subgraphs and utilizes **GCN layers** to enhance semantic preservation and improve similarity search accuracy.

## Experimental Results
- Achieved superior **mAP performance** compared to existing CNN and GCN-based hashing methods across multiple datasets.  
- GCNs, despite their simplicity, provided competitive results with lower computational complexity.  
- HAGCN effectively reduces quantization error between continuous and binary embeddings and performs well even in **unsupervised (label-scarce) environments**.

## Applicability
The improved model shows strong potential for use in real-world applications requiring **fast and accurate similarity search**.

## Limitations
- **High computational cost** of GCNs, especially on large-scale datasets  
- Need for **further validation** across diverse data types

## Future Work
- Explore optimization techniques to improve **computational efficiency** of GCN-based hashing  
- Conduct broader **performance validation on real-world datasets**
